https://sebastianraschka.com/Articles/2014_about_feature_scaling.html

Feature scaling - If left alone, these algorithms only take in the magnitude of features neglecting the units. 
                  The results would vary greatly between different units, 5kg and 5000gms. 
                  The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.

Some examples of algorithms where feature scaling matters are:
 - k-nearest neighbors with an Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally.
 - Scaling is critical, while performing Principal Component Analysis(PCA). PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.
 - We can speed up gradient descent by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.
 - Tree based models are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees.
 - Algorithms like Linear Discriminant Analysis(LDA), Naive Bayes are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect.

Standadization

1. Standadization - mean 0 and std =1

2.  Features will be rescaled so that they’ll have the properties of a standard normal distribution
	i.e mean 0 and std =1

3.  But it is also a general requirement for many machine learning algorithms. 
    Intuitively, we can think of gradient descent as a prominent example 
    (an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.)

4. The only family of algorithms that I could think of being scale-invariant are tree-based methods



Normalization

1. The data is scaled to a fixed range - usually 0 to 1.
2. A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). 
   Also, typical neural network algorithm require data that on a 0-1 scale.